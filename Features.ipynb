{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime as datetime\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the Haversine distance between two points on the Earth.\n",
    "    Args:\n",
    "        lat1, lon1: Latitude and longitude of the first location (cardholder).\n",
    "        lat2, lon2: Latitude and longitude of the second location (merchant).\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Assuming your DataFrame is named `df`\n",
    "# Ensure 'trans_date' and 'trans_time' are in proper datetime formats\n",
    "df['trans_datetime'] = pd.to_datetime(df['trans_date'] + ' ' + df['trans_time'])\n",
    "df['distance'] = df.apply(\n",
    "    lambda row: haversine_distance(row['lat1'], row['long1'], row['lat2'], row['long2']), axis=1\n",
    ")\n",
    "# Group by credit card number (`cc_num`) to compute user-specific features\n",
    "grouped = df.groupby('cc_num')\n",
    "\n",
    "# --- Transaction Amount Features ---\n",
    "# User average and standard deviation of transaction amounts\n",
    "df['user_avg_amt'] = grouped['amt'].transform('mean')\n",
    "df['user_std_amt'] = grouped['amt'].transform('std')\n",
    "df['deviation_from_mean_amt'] = df['amt'] - df['user_avg_amt']\n",
    "df['amt_z_score'] = (df['amt'] - df['user_avg_amt']) / df['user_std_amt']\n",
    "\n",
    "# --- Time-Based Features ---\n",
    "df['time_since_last_trans'] = grouped['trans_datetime'].transform(lambda x: x.diff().dt.total_seconds())\n",
    "df['avg_time_between_trans'] = grouped['time_since_last_trans'].transform('mean')\n",
    "df['deviation_from_mean_time'] = df['time_since_last_trans'] - df['avg_time_between_trans']\n",
    "\n",
    "# Day and hour of the transaction\n",
    "df['day_of_week'] = df['trans_datetime'].dt.dayofweek\n",
    "df['hour_of_day'] = df['trans_datetime'].dt.hour\n",
    "\n",
    "# Binary flag for unusual frequency (transactions within a short interval, e.g., 5 minutes)\n",
    "df['unusual_frequency'] = df['time_since_last_trans'] < 300\n",
    "\n",
    "# --- Category-Based Features ---\n",
    "# Previous transaction category for each user\n",
    "df['prev_category'] = grouped['category'].shift(1).astype('category')\n",
    "df['next_category'] = grouped['category'].shift(-1).astype('category')\n",
    "df['same_category_as_prev'] = df['prev_category'] == df['category']\n",
    "df['same_category_as_next'] = df['next_category'] == df['category']\n",
    "df['at_least_one_same_category'] = df['same_category_as_prev'] | df['same_category_as_next']\n",
    "\n",
    "# df['category_transition'] = df['prev_category'] + '->' + df['category']\n",
    "\n",
    "# Frequency of category transitions\n",
    "# category_transition_counts = df.groupby(['cc_num', 'category_transition'])['trans_num'].count().reset_index(name='transition_count')\n",
    "# df = df.merge(category_transition_counts, on=['cc_num', 'category_transition'], how='left')\n",
    "\n",
    "# Category entropy for the user's transactions\n",
    "def calculate_entropy(categories):\n",
    "    probs = categories.value_counts(normalize=True)\n",
    "    return -np.sum(probs * np.log(probs))\n",
    "\n",
    "df['category_entropy'] = grouped['category'].transform(calculate_entropy)\n",
    "\n",
    "# --- General Features ---\n",
    "# Distance between transaction location and merchant location\n",
    "df['distance_to_merchant'] = np.sqrt((df['lat'] - df['merch_lat'])**2 + (df['long'] - df['merch_long'])**2)\n",
    "\n",
    "# Is the transaction on a weekend?\n",
    "df['is_weekend'] = df['trans_datetime'].dt.dayofweek >= 5\n",
    "\n",
    "# --- Rolling Features ---\n",
    "# Rolling window for transaction amounts\n",
    "window_size = 5\n",
    "df['rolling_avg_amt'] = grouped['amt'].transform(lambda x: x.rolling(window_size, min_periods=1).mean())\n",
    "df['rolling_std_amt'] = grouped['amt'].transform(lambda x: x.rolling(window_size, min_periods=1).std())\n",
    "\n",
    "# Rolling count of category changes\n",
    "df['rolling_category_changes'] = grouped['category'].transform(lambda x: (x != x.shift(1)).rolling(window_size, min_periods=1).sum())\n",
    "\n",
    "# Rolling time between transactions\n",
    "df['rolling_time_between_trans'] = grouped['time_since_last_trans'].transform(lambda x: x.rolling(window_size, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     98521\n",
      "           1       0.88      0.64      0.74     12690\n",
      "\n",
      "    accuracy                           0.95    111211\n",
      "   macro avg       0.92      0.81      0.86    111211\n",
      "weighted avg       0.95      0.95      0.95    111211\n",
      "\n",
      "ROC-AUC Score: 0.9312700342398191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "features_to_use = ['user_avg_amt', 'user_std_amt', 'deviation_from_mean_amt', 'amt_z_score', \n",
    "                   'time_since_last_trans', 'avg_time_between_trans', 'deviation_from_mean_time', \n",
    "                   'day_of_week', 'hour_of_day', 'unusual_frequency', 'prev_category', 'next_category',\n",
    "                   'same_category_as_prev', 'same_category_as_next', 'at_least_one_same_category',\n",
    "                   'category_entropy', 'distance_to_merchant', 'is_weekend', \n",
    "                   'rolling_avg_amt', 'rolling_std_amt', 'rolling_category_changes', \n",
    "                   'rolling_time_between_trans']\n",
    "bool_columns = ['unusual_frequency', 'same_category_as_prev', 'same_category_as_next', \n",
    "                'at_least_one_same_category', 'is_weekend', 'day_of_week_1', 'day_of_week_2', \n",
    "                'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6']\n",
    "\n",
    "X = df[features_to_use]\n",
    "y = df['is_fraud']\n",
    "\n",
    "# One-hot encode categorical columns if necessary\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "bool_columns = X_train.select_dtypes(include=['bool']).columns\n",
    "X_train[bool_columns] = X_train[bool_columns].astype(int)\n",
    "X_train = pd.get_dummies(X_train, columns=['prev_category', 'next_category'], drop_first=True)\n",
    "X_train.fillna(0, inplace=True)\n",
    "\n",
    "bool_columns_test = X_test.select_dtypes(include=['bool']).columns\n",
    "X_test[bool_columns_test] = X_test[bool_columns_test].astype(int)\n",
    "X_test = pd.get_dummies(X_test, columns=['prev_category', 'next_category'], drop_first=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "# Initialize Decision Tree\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    criterion='gini',  # 'entropy' for information gain\n",
    "    max_depth=5,       # Limit tree depth for better generalization\n",
    "    min_samples_split=10,  # Minimum samples required to split an internal node\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the Decision Tree\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = dt_model.predict_proba(X_test)[:, 1]  # Probability of being fraud\n",
    "y_pred = dt_model.predict(X_test)         \n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.read_csv('train.csv')\n",
    "final_test = pd.read_csv('test.csv')\n",
    "final_df = pd.concat([final_train, final_test], axis=0)\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is named `df`\n",
    "# Ensure 'trans_date' and 'trans_time' are in proper datetime formats\n",
    "final_df['trans_datetime'] = pd.to_datetime(final_df['trans_date'] + ' ' + final_df['trans_time'])\n",
    "\n",
    "# Group by credit card number (`cc_num`) to compute user-specific features\n",
    "grouped = final_df.groupby('cc_num')\n",
    "\n",
    "# --- Transaction Amount Features ---\n",
    "# User average and standard deviation of transaction amounts\n",
    "final_df['user_avg_amt'] = grouped['amt'].transform('mean')\n",
    "final_df['user_std_amt'] = grouped['amt'].transform('std')\n",
    "final_df['deviation_from_mean_amt'] = final_df['amt'] - final_df['user_avg_amt']\n",
    "final_df['amt_z_score'] = (final_df['amt'] - final_df['user_avg_amt']) / final_df['user_std_amt']\n",
    "\n",
    "# --- Time-Based Features ---\n",
    "final_df['time_since_last_trans'] = grouped['trans_datetime'].transform(lambda x: x.diff().dt.total_seconds())\n",
    "final_df['avg_time_between_trans'] = grouped['time_since_last_trans'].transform('mean')\n",
    "final_df['deviation_from_mean_time'] = final_df['time_since_last_trans'] - final_df['avg_time_between_trans']\n",
    "\n",
    "# Day and hour of the transaction\n",
    "final_df['day_of_week'] = final_df['trans_datetime'].dt.dayofweek\n",
    "final_df['hour_of_day'] = final_df['trans_datetime'].dt.hour\n",
    "\n",
    "# Binary flag for unusual frequency (transactions within a short interval, e.g., 5 minutes)\n",
    "final_df['unusual_frequency'] = final_df['time_since_last_trans'] < 300\n",
    "\n",
    "# --- Category-Based Features ---\n",
    "# Previous transaction category for each user\n",
    "final_df['prev_category'] = grouped['category'].shift(1).astype('category')\n",
    "final_df['next_category'] = grouped['category'].shift(-1).astype('category')\n",
    "final_df['same_category_as_prev'] = final_df['prev_category'] == final_df['category']\n",
    "final_df['same_category_as_next'] = final_df['next_category'] == final_df['category']\n",
    "final_df['at_least_one_same_category'] = final_df['same_category_as_prev'] | final_df['same_category_as_next']\n",
    "\n",
    "# final_df['category_transition'] = final_df['prev_category'] + '->' + final_df['category']\n",
    "\n",
    "# Frequency of category transitions\n",
    "# category_transition_counts = final_df.groupby(['cc_num', 'category_transition'])['trans_num'].count().reset_index(name='transition_count')\n",
    "# final_df = final_df.merge(category_transition_counts, on=['cc_num', 'category_transition'], how='left')\n",
    "\n",
    "# Category entropy for the user's transactions\n",
    "def calculate_entropy(categories):\n",
    "    probs = categories.value_counts(normalize=True)\n",
    "    return -np.sum(probs * np.log(probs))\n",
    "\n",
    "final_df['category_entropy'] = grouped['category'].transform(calculate_entropy)\n",
    "\n",
    "# --- General Features ---\n",
    "# Distance between transaction location and merchant location\n",
    "final_df['distance_to_merchant'] = np.sqrt((final_df['lat'] - final_df['merch_lat'])**2 + (final_df['long'] - final_df['merch_long'])**2)\n",
    "\n",
    "# Is the transaction on a weekend?\n",
    "final_df['is_weekend'] = final_df['trans_datetime'].dt.dayofweek >= 5\n",
    "\n",
    "# --- Rolling Features ---\n",
    "# Rolling window for transaction amounts\n",
    "window_size = 5\n",
    "final_df['rolling_avg_amt'] = grouped['amt'].transform(lambda x: x.rolling(window_size, min_periods=1).mean())\n",
    "final_df['rolling_std_amt'] = grouped['amt'].transform(lambda x: x.rolling(window_size, min_periods=1).std())\n",
    "\n",
    "# Rolling count of category changes\n",
    "final_df['rolling_category_changes'] = grouped['category'].transform(lambda x: (x != x.shift(1)).rolling(window_size, min_periods=1).sum())\n",
    "\n",
    "# Rolling time between transactions\n",
    "final_df['rolling_time_between_trans'] = grouped['time_since_last_trans'].transform(lambda x: x.rolling(window_size, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/h9nxt46j4r7dc2xptjh4w0300000gn/T/ipykernel_4856/760141285.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_testing[bool_columns_final] = final_testing[bool_columns_final].astype(int)\n"
     ]
    }
   ],
   "source": [
    "final = final_df[final_df['id'].isin(final_test['id'])]\n",
    "final_testing = final[features_to_use]\n",
    "\n",
    "bool_columns_final = final_testing.select_dtypes(include=['bool']).columns\n",
    "final_testing[bool_columns_final] = final_testing[bool_columns_final].astype(int)\n",
    "final_testing = pd.get_dummies(final_testing, columns=['prev_category', 'next_category'], drop_first=True)\n",
    "final_testing.fillna(0, inplace=True)\n",
    "\n",
    "y_pred_prob = dt_model.predict_proba(final_testing)[:, 1]  # Probability of being fraud\n",
    "y_pred = dt_model.predict(final_testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({\n",
    "    'id': final['id'],\n",
    "    'is_fraud': y_pred\n",
    "})\n",
    "\n",
    "# Writing to a CSV file\n",
    "output_df.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
